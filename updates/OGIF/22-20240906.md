---
tags:
  - office-hours
  - ogif
  - discord
title: "OGIF Office Hours #22 - Hybrid working, Tech market report, Go commentary weekly, AI demo for Go weekly content production."
short_title: "#22 Hybrid work, Tech report, Go weekly, AI demo"
date: 2024-09-09
description: In OGIF 22, we talked about Devbox progress, GReader updates, and using libre chat for artifact management. We also cover optimizing AI system prompts, Go commentary, and key market trends. Plus, we discuss how hybrid work can boost team collaboration and productivity. It’s all about practical tips and knowledge sharing to keep everyone up to speed.
authors:
  - innno_
---

100 minutes

### Topics & Highlights
- Office improvements announced for remote working.
- Market report shared with project updates.
- Live demo on software research and development.
- Emphasis on team collaboration and social connections.
- New AI tools showcased for enhanced productivity.
- Discussion on Go programming updates and features.
- Audience engagement encouraged throughout the session.

---

**Vietnamese Transcript**

**00:00** Đây rồi, còn thêm ai nữa không? Vậy là hôm nay mình có ba tiết mục. Đầu tiên là market report của Thành, sau đó là phần phát triển, chắc sẽ nhanh thôi. Phần còn lại dành cho Tôm. Đấy, vẫn còn nhiều thời gian quá. Bài của Mỹ cũng nhiều, để không thì Mỹ post luôn, rồi anh em xem sau nhé. Mỹ ơi, em post bài của mình chưa? Mọi người có thấy màn hình chưa? Dạ, tiếng hơi nhỏ à? Đợi chút để mình lấy phone ra. Giờ mình bắt đầu nhé. Phần đầu tiên là về communities. 

**17:39** Ngoài các con số tuần trước, chắc anh sẽ đi qua một số điểm tin. Thông tin quan trọng là từ tuần sau, team mình sẽ bắt đầu triển khai hai việc. Thứ nhất là enable hybrid working. Tí nữa anh sẽ gửi link chi tiết, nhưng hiện tại thì chủ yếu vẫn là work through một chút. Office sẽ được cải tiến dành cho những bạn cảm thấy làm việc tại nhà hơi ồn, hoặc không tập trung được. 

<iframe width="560" height="315" src="https://www.youtube.com/embed/wED1g78PEn0?si=Ne0XOi-_aDTNzuST&amp;start=1087" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

**18:31** Hiện tại Office của mình có khoảng năm người đang sử dụng, vẫn còn trống khoảng mười chỗ nữa. Khi mọi người lên đây, mình sẽ được cover chi phí sử dụng Apple Studio Display, ghế Herman Miller cũng đã được trang bị đầy đủ. Nếu anh em chưa biết thì có thể Google để hiểu rõ về ghế này. Phòng họp và một số trang thiết bị khác trong Office cũng đã được cải tiến đáng kể. Những bạn nào cảm thấy ở nhà chán hoặc không thoải mái, có thể cân nhắc đến Office làm việc một hai ngày trong tuần. Điều này là optional, không bắt buộc.

**19:48** Nhưng team mong muốn mọi người bắt đầu điều chỉnh công việc, để nó mang lại những lợi ích thiết yếu cho việc làm việc chung tại văn phòng. Tăng cường các mối quan hệ xã hội, đặc biệt trong giai đoạn này khi lượng kiến thức và nghiên cứu cần xử lý rất lớn. Việc làm chung với nhau sẽ giúp đẩy nhanh tiến độ, nhất là khi chúng ta đang ở giai đoạn cuối các dự án.

**20:28** Ngoài ra, tất cả chi phí về parking và bữa ăn tối sẽ được hỗ trợ. Đây là thông báo chính mà tí nữa Mỹ sẽ viết thông báo chi tiết cho tất cả mọi người. Đó là thông báo đầu tiên, và anh nghĩ mọi người sẽ quan tâm đến nó. Thông báo thứ hai là về việc test thử một tính năng mới: Daily check-in. Nếu ai tham gia Daily check-in, trong hai tuần đầu tiên, mỗi khi check-in sẽ được thưởng khoảng từ 3 đến 5 ICY.

**22:29** Đúng rồi, mọi người sẽ nhận được ICY mỗi ngày. Nhưng trong giai đoạn ban đầu, tụi anh sẽ chạy thử nghiệm trong khoảng hai tuần để xem tính năng này hoạt động như thế nào. Vì số lượng chỗ trong Office có giới hạn, nên anh sẽ ưu tiên cho những ai bắt đầu hybrid working và còn dư chỗ thì sẽ chuyển dần remote. Để cải tiến chất lượng làm việc tại Office, tụi anh đang cố gắng update Office lên mức tốt nhất có thể.

**23:24** Vì thực tế là số chỗ trong Office có hạn, nên ưu tiên là khuyến khích những ai muốn lên Office vài ngày mỗi tuần, nếu cảm thấy làm việc tại nhà không hiệu quả. Và nếu sau một hai tuần mọi người thấy thích nghi tốt, thì có thể personalization góc làm việc riêng của mình, mang những món đồ cá nhân lên để tạo cảm giác thoải mái hơn.

**24:04** Team Hà Nội thì check-in ở đâu? Ừ, anh cũng mang tủ cá nhân và vài món đồ Lego của con lên Office luôn rồi. Đấy là hai thông báo chính. Hy vọng mọi người sẽ cân nhắc và bắt đầu thử nghiệm. Tụi anh đang cố gắng khôi phục lại văn hóa làm việc vật lý, với những ngày hybrid working như 1-2 ngày/tuần tùy vào trạng thái tâm lý và sự thoải mái của mỗi người.

**25:15** Rồi, mọi người sẽ thấy rằng việc lên văn phòng sẽ giúp có nhiều kết nối cá nhân hơn. Có thể đi làm nửa ngày hoặc cả ngày cũng được, tùy vào tình hình của mọi người. Ok, vậy là xong phần quan trọng rồi nhé. Giờ anh sẽ chuyển sân khấu lại cho Thành để tiếp tục phần market report.

**26:08** Bây giờ, chúng ta sẽ đi qua những bài viết đáng chú ý trong tuần qua. Top 20 bài đã được đọc và chuyển từ DynamoDB sang MySQL, và có một bài viết khá hay về Neil, hỗ trợ cho bài nghiên cứu về Debox. Hôm qua mọi người có thảo luận khá sôi nổi về Debox trên kênh chat của team tech. Nội dung thảo luận đã được ghi lại, mọi người có thể vào xem. Nhưng có một số điểm chưa rõ về Debox, nó chỉ là một môi trường development box, không phải là môi trường deployment như container.

<iframe width="560" height="315" src="https://www.youtube.com/embed/wED1g78PEn0?si=YvI91WKNTGQxoUP7&amp;start=1965" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

**27:11** Debox chỉ là một box cho môi trường development thôi, nó không phải là môi trường deployment như container. Về cơ bản, chúng ta vẫn sẽ cần cả hai môi trường này vì chúng không thay thế cho nhau. Trên môi trường development, đặc biệt là đối với những người sử dụng máy M1, thì Debox sẽ cài một layer VM trên máy M1 để có thể chạy container. Điều này là do Debox không thể chạy native trên M1 mà phải qua một lớp trung gian là VM. Giải pháp mà NX team đang sử dụng là Linux trên VM, và nó được cài đặt trực tiếp trên máy, không có tầng trung gian giữa như các hệ thống khác. Đó là lý do tại sao hệ thống này hoạt động tốt trên các máy M1.

**27:47** Bài tiếp theo nói về Accessibility Design trong quá trình sử dụng Figma. Link bài này có vẻ bị sai hết rồi, Huy ơi. Nếu Huy có ở đây thì xem lại và sửa giúp phần link cho đúng nhé. Em có thể post lại đường link của những người đã đăng bài trước đó để mọi người tiện theo dõi. Hình như có ba cái link bị sai rồi, em sửa lại nhé.

**29:12** Còn về YouTube thì không có, nhưng Discord thì có. Đúng rồi, React 19 và hệ thống log cho developer bằng Go đã được đăng ký. Cái này là hệ thống log system viết bằng Go, có tích hợp với Relay App. Hôm qua đã có một bạn đăng ký thử, nhưng có vẻ là nhầm link. Sáng hôm qua có một bạn post về Data Engineering cho Python, nhưng đây là giải pháp dành cho thay thế Python thôi. Còn hệ thống của anh thì vẫn đang sử dụng Shell Script, không phải Python.

**30:38** Inno cũng có một bài rất hay về bản quyền của OpenAI. Đây là tin tức trong tuần qua, mọi người có thể xem qua các bài báo cáo của Tom. Những bài báo cáo của Tô đã tổng hợp lại nhiều thông tin quan trọng trong tuần này. Ba người đã đăng link lại rồi, Tô có thể giúp post lại các bài đó không? Những bài đó đều nằm trên Facebook của chúng ta. Mọi người có thể vào xem lại, và kéo view cho các bài đó để giúp tăng tương tác. Đặc biệt, bài của Huy mới post hôm qua có hình ảnh rất đẹp và gọn gàng hơn rất nhiều so với trước.

**32:48** Để mình xem mọi người đã thấy màn hình chưa? Tuần này cũng không có quá nhiều thông tin, chỉ có hai bài chính thôi. Bài đầu tiên là Script, đây là một tool cho phép viết các script trong chương trình Go. Cách sử dụng nó cũng khá đơn giản, ví dụ như đọc một file thành chuỗi (string), hay đếm các dòng khớp với một mẫu nào đó. Thậm chí chúng ta có thể nhúng (embed) các hàm Go vào trong nó luôn. Công cụ này khá là mới, và nó cung cấp rất nhiều tính năng tùy chỉnh. Có thể gọi request HTTP, hoặc thậm chí là custom các hàm của chính mình.

**33:40** Ví dụ như đoạn script bên dưới cho phép đọc một file và đếm các dòng khớp với một chuỗi ký tự nhất định. Ngoài ra, nó còn hỗ trợ việc nhúng các function của Go vào trong script đó. Đây là một công cụ mới, giống như công cụ Jam mà mọi người đã biết trước đây, nhưng Jam chỉ là công cụ cho việc viết các shell script đơn giản. Còn công cụ này không tập trung vào UI, nó chỉ chủ yếu là API.

**34:28** Công cụ thứ hai là về Go CH, bắt đầu từ bản 1.23, chúng ta có thể bật hoặc tắt (toggle) telemetry. Telemetry là việc thu thập dữ liệu sử dụng của ứng dụng, ví dụ như app của chúng ta đang hoạt động thế nào, có bị crash hay không. Dữ liệu này sẽ được gửi về cho team Go, để họ có thể theo dõi và cải thiện. Nếu mọi người follow kênh Slack của Go thì có thể thấy một số lỗi được phát hiện nhờ vào telemetry này. Với bản 1.23, chúng ta có thể tắt nó đi hoặc giữ mặc định là chỉ gửi dữ liệu local, không gửi lên server.

**35:08** Nếu mọi người tham gia kênh Slack của Go thì sẽ thấy rằng có một số lỗi đã được pop-up nhờ vào dữ liệu từ telemetry này. Do đó, từ bản 1.23 trở đi, mọi người có thể bật hoặc tắt nó, hoặc nếu không muốn gửi dữ liệu đi, có thể chỉ giữ lại ở chế độ local. Bài này của tuần này chỉ có vậy thôi.

**35:54** Phát có một câu hỏi, cái code này có phải là hệ thống mà Huy đã set up cho mấy dự án của mình không? Em không biết nữa. Cái Huy set up là gì vậy, có phải là hệ thống error reporting không? Ừ, đúng rồi, error reporting. Nhưng hệ thống error bot này là một công cụ của Go team, nó sẽ gửi các báo cáo lỗi thẳng về cho team Go. Mục đích chính là để cải thiện Go, xem xem nó có lỗi gì hay không.

**36:36** Nó bắt được các bug trong ngôn ngữ lập trình Go, đặc biệt là khi chương trình bị crash hoặc khi các công cụ bên ngoài như Go Please hay Go RoboCheck gặp lỗi. Nó sẽ gửi báo cáo lỗi về cho team Go để họ có thể nắm bắt và sửa lỗi. Hệ thống này chỉ là một tool cho team Go, nó không phải là công cụ chung để mọi người dùng đâu.

**37:24** Phát có hỏi rằng có nên enable (kích hoạt) hệ thống này không? Nếu kích hoạt, chúng ta sẽ gửi một lượng dữ liệu nhỏ về cho server của Go. Tuy nhiên, em nghĩ lượng dữ liệu này rất nhỏ, chỉ là các file về hiệu suất hoạt động thôi. Nó cũng sẽ tích hợp với hệ thống profile guided optimization của Go mà em đã nói lần trước. Những dữ liệu này sẽ giúp cải thiện hiệu suất của ứng dụng mà không tốn quá nhiều chi phí.

**38:08** Phần dữ liệu này chủ yếu là file hiệu suất, thường chỉ là một vài file nhỏ thôi. Nếu mọi người không muốn gửi dữ liệu đi, có thể tắt hoàn toàn hệ thống telemetry này. Trong môi trường production, chúng ta có thể set nó ở chế độ local để không gửi gì cả.

**38:51** Hệ thống profile mà Phát nói lần trước là build một lần và sau đó đo lại hiệu suất trong những lần build sau. Nó sẽ kiểm tra xem hiệu suất cải thiện như thế nào so với lần build trước. Cảm ơn Phát đã chia sẻ thông tin. Hằng có thể tổng hợp lại những phần báo cáo của tháng trước không? Nhất là những phần quan trọng về hiệu suất của hệ thống trong tháng vừa rồi.

**40:10** Mọi người có thể thấy rằng lượng thông tin khá lớn, nhưng chắc là mọi người cũng đã nắm được. Hệ thống Wind mới cũng đã có một số cải tiến đáng kể. Còn về vấn đề lỗi thì chắc là sẽ phải xử lý thêm. Cái của Quân Lê đã hoàn thành chưa? Đúng rồi, đã thấy rồi, nhưng hình như còn thiếu một số file cuối cùng. Hằng có thể tổng hợp lại cho team để hoàn thành báo cáo không?

**41:43** Nhìn vào những công cụ AI mới trong tháng vừa rồi, có một số sự phát triển đáng chú ý, đặc biệt là công cụ CR 3.5 và Sonnet. Trước đây, chúng ta đã thấy demo về chức năng của chúng, nhưng bây giờ chúng đã đi vào thực tế, không chỉ là demo nữa. Một số team đã bắt đầu sử dụng chúng để phát triển ứng dụng. Công cụ CR 3.5 Sonnet này cũng đã được Amazon thử nghiệm cho việc nâng cấp hệ thống Java lên phiên bản mới hơn.

<iframe width="560" height="315" src="https://www.youtube.com/embed/wED1g78PEn0?si=B7F-sKuHfX5_3AcW&amp;start=2461" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

**43:12** Theo một báo cáo chưa xác thực, công cụ này có thể xử lý khoảng 80-90% công việc trong quá trình nâng cấp hệ thống, và chi phí tiết kiệm được là khá lớn. Đây là một dấu hiệu cho thấy các công cụ AI đã bắt đầu tham gia vào quá trình phát triển phần mềm một cách thực tế hơn. Ngoài ra, các chatbot hay công cụ AI khác cũng đang có xu hướng thêm những chức năng như artifact, cho phép chúng compile các đoạn mã nhận từ người dùng và tạo ra các mini app trong môi trường chat.

**44:42** Ví dụ, với CR 3.5 Sonnet, có một form cho phép người dùng tính toán chi phí xây dựng, và khi nhập dữ liệu vào, hệ thống sẽ generate ra một đoạn mã HTML và tạo ra một ứng dụng web nhỏ để xử lý dữ liệu đó. Trước đây, việc này chỉ liên quan đến UI, nhưng bây giờ nó đã phát triển thành khả năng tạo ra các ứng dụng mini ngay trong giao diện chat. Tô có thể demo đoạn này cho mọi người xem sau.

**45:28** Bên cạnh đó, hệ thống ping của Cross cũng đang được phát triển, giúp tiết kiệm tài nguyên bằng cách giảm bớt việc request lại phần code đã submit trước đó. Chức năng này giúp giảm độ trễ (latency) và cải thiện tốc độ xử lý.

**46:16** Một điểm đáng chú ý khác là model TT4 của OpenAI đã bắt đầu hỗ trợ chức năng sure output, đảm bảo rằng output cuối cùng luôn đúng với schema đã định trước. Trước đây, việc này phụ thuộc rất nhiều vào việc định nghĩa schema từ đầu, nhưng bây giờ hệ thống sẽ tự động kiểm tra và đảm bảo rằng output cuối cùng luôn đúng. Điều này rất hữu ích trong những hệ thống phụ thuộc nhiều vào schema và có tỷ lệ lỗi cao.

**47:31**  Ví dụ như ở đây, nếu mình phát triển một ứng dụng mà kiểu dạng như vậy, nó sẽ liên quan đến rất nhiều yếu tố, như là phải kiểm tra chính xác các phần tử selector trong source. Chẳng hạn như mình phải input đúng selector của nó là gì, selector của link này ra sao, title của link này là gì. Để mà có thể scan đúng được tất cả các link đó. Thực ra mỗi trang web sẽ có một cấu trúc khác nhau, làm sao để có một giải pháp tổng thể? Thực tế, AI có thể giúp mình hiểu được cấu trúc HTML của trang web đó, hiểu được cách nó bố trí các phần tử. Và đây là một công cụ nhỏ nhỏ, hỗ trợ việc này. Nhưng mà mình sẽ cần phải đảm bảo rằng output cuối cùng của nó luôn luôn là danh sách các link mà mình scan được.

Hồi trước, để làm được việc này thì sẽ cần rất nhiều bước liên quan đến việc ping các thứ. Nhưng bây giờ, với AI, chúng ta có thể define sẵn schema ở đây. Trong Visual Studio Code chẳng hạn, anh em có thể định nghĩa schema trước, và đảm bảo rằng output cuối cùng sẽ luôn luôn có đầy đủ title, luôn có đầy đủ link, và luôn có phần mô tả (description). Điều này sẽ đảm bảo rằng khi chúng ta quét các link, mọi thứ sẽ chính xác. Đây là một ví dụ về output khi chúng ta quét các link từ Lobs, hoặc một trang web tương tự.

**48:44** Điều này là một update khá quan trọng trong giai đoạn vừa qua mà mọi người nên biết. Ngoài ra, DVT4 cũng hỗ trợ việc tinh chỉnh file, tức là chúng ta có thể fine-tune model dựa trên dữ liệu cá nhân. Điều này rất hữu ích khi mình muốn train AI trên các bộ dữ liệu riêng của mình để phục vụ cho mục đích cụ thể nào đó. Hiện tại, tính năng này đã được cung cấp và có thể sử dụng. Bên cạnh đó, có thêm một module mới liên quan đến việc xử lý hình ảnh (image processing) cũng như flagging, và một số tính năng khác, chắc là mọi người đã xem qua rồi.

**49:27** Ngoài ra, còn có một cập nhật liên quan đến phần UI, hiện tại team mình đang rất hứng thú với công cụ V0. Đây là một tool mới, giúp generate ra các phần UI dựa trên prompt (lời nhắc). Kết quả ban đầu từ tool này khá là sát với kỳ vọng của team mình. Nhiều team đã bắt đầu sử dụng nó để build các mini app. Điều đáng chú ý là V0 hiện đang được train với hai framework chính, đó là Svelte và TailwindCSS. Công cụ này đã có trên thị trường một thời gian rồi, và team mình đã dùng nó khoảng một năm nay. Các team khác cũng đã bắt đầu tích hợp nó vào quy trình của họ.

**50:22** V0 có ưu điểm là kích thước thư viện rất nhỏ. Nó dựa trên Svelte UI và kết hợp với TailwindCSS, nên việc tùy chỉnh UI và các component rất dễ dàng. Sự kết hợp giữa hai công nghệ này đã giúp V0 trở thành một lựa chọn rất được ưa chuộng trong việc prototyping (tạo mẫu) các component UI. Chúng ta kỳ vọng rằng trong tương lai gần, V0 sẽ chiếm lĩnh thị phần của các thư viện lớn như Material UI hay Ant Design.

**51:10** Trong tháng vừa rồi, có một số cập nhật như vậy về công cụ và các tính năng mới. Không biết mọi người có câu hỏi nào không? Về tổng thể, tháng này các bản tin vẫn chủ yếu xoay quanh những chủ đề tactical (chiến thuật) về việc nâng cấp hệ thống và các công cụ đang trending mà các bên lớn như Amazon và Google đang phát triển. Hiện tại thì chưa có nhiều tin tức về các sản phẩm mới hay công nghệ hoàn toàn đột phá, nhưng chúng ta cũng đã thấy những bước tiến nhất định.

**51:59** Những tin tức gần đây bắt đầu có dấu hiệu về sự tích hợp sâu hơn của các hệ thống AI vào ứng dụng thực tế. Ví dụ như Apple đang tích hợp trực tiếp các công cụ AI vào hệ thống, hoặc Amazon đã gỡ bỏ Alexa để thay thế bằng AI cloud-based (dựa trên đám mây). Không biết sắp tới sẽ có những thay đổi như thế nào, nhưng xu hướng rõ ràng là các doanh nghiệp lớn đang dần dần đưa AI vào quy trình của họ một cách toàn diện hơn.

**53:01** Ngày hôm qua, khi team mình kiểm tra email, Google cũng đã gửi thông báo về việc muốn enable AI cho team này hay không. Họ đang cung cấp dịch vụ với mức giá khoảng 15 đô la/người, có những gói giá khác nhau nhưng nhìn chung là khá đắt. Mình thì chỉ xài AI ở mức độ vừa phải thôi, không quá nhiều.

**53:49** Tuy nhiên, chúng ta có thể mong đợi rằng giá của các dịch vụ AI sẽ giảm dần theo thời gian, giống như lần trước có một bài báo cáo cho thấy chi phí sử dụng GPT-3.5 năm ngoái là khoảng 32 đô la, nhưng năm nay khi sử dụng GPT-4 thì giá đã giảm gần một nửa. Số lượng token (đơn vị tính toán) cũng ngày càng tăng, dẫn đến việc giá thành giảm xuống. Có lẽ trong tương lai gần, những phương pháp cũ như RNN (Recurrent Neural Networks) sẽ dần trở nên lỗi thời, và AI sẽ ngày càng phổ cập hơn.

**54:34** Bên phía Tony Dinh cũng đang phát triển một hệ thống tương tự với công cụ mà chúng ta đang dùng, nhưng tốc độ của họ không nhanh bằng, nên khả năng họ sẽ không cạnh tranh nổi. Họ đang bán một công cụ tên là TimingMind, một hệ thống giúp các doanh nghiệp tự sử dụng dữ liệu của mình để phát triển các công cụ AI nội bộ. Mục tiêu của họ là đưa toàn bộ hệ thống Office, các công cụ làm việc, và các dịch vụ hỗ trợ đi kèm với AI.

**55:44** Điều thú vị là hiện tại, họ đang chạy đua với tốc độ phổ cập AI, bán ra thị trường những gì có thể bán ngay bây giờ. Về lâu dài, AI sẽ trở thành một phần không thể thiếu trong các sản phẩm phần mềm, giống như ngành công nghiệp phần mềm thông thường, với những yêu cầu ngày càng cao về chi phí, hiệu suất và dịch vụ khách hàng.

**56:48** Team Google cũng có một dịch vụ hỗ trợ khách hàng rất chuyên nghiệp, họ dùng cả AI để tương tác với khách hàng. Khá ấn tượng khi họ sử dụng AI của chính họ để làm việc này, và kết quả là rất trơn tru, không khác gì nói chuyện với một người thật. Được rồi, chúng ta sẽ xin cái link kia để mọi người có thể xem và tiếp tục thảo luận.

**57:37** chắc nhờ Phát lên pair program với em. Thật sự là hôm nay mình sẽ làm một cái workflow chung với Phát. Thật sự thì em gần như làm hết, nhưng sẽ đưa ra một cái hướng suy nghĩ về lập trình cái app kiểu này. Bên phía anh Thành cũng có đề cập rằng mình muốn tạo ra nhiều cái app viết trên cloud, mình có thể viết ra rất nhiều app khá là xịn đấy. Hôm nay thì mình sẽ chơi Dify, nghĩa là hôm nay sẽ có một cái Plan và Plan là thay thế Phát, tự cho mọi người viết được app kiểu như vậy.

<iframe width="560" height="315" src="https://www.youtube.com/embed/wED1g78PEn0?si=-WbmpDSPizo0PBRS&amp;start=3501" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

**58:23** Go commentary, thì ban đầu bên phía em sẽ có một cái Plan là tạo system prompt nhờ AI, tạo system prompt thêm và gán input. Input ở đây là link, output là bài viết và note. Ở đây thì cần thiết một cái agent để lấy link, sau đó là từ cái này thì sẽ summarize link, format lại, note lại. Đây là suy nghĩ từ bên phía em trước khi bắt đầu đoạn work. Mình sẽ chơi luôn, cho mọi người nhìn lại cái cấu trúc này. Tức là làm một cái bài Go commentary hả? Dạ, đúng rồi. Copy toàn bộ cái này luôn, tức là làm Go commentary thay cho Phát. Yeah, thì thay thế được mà.

**59:22** Mình sẽ cần AI để tạo ra system prompt như vậy. Em đã tạo một chatbot, đúng ra là có thể dùng CL nhưng mình đang dùng library chat để tạo cho AI Club rồi. Hôm nay thì nó sẽ là: "Help me create a system prompt to format note in Markdown format with the following style..." Sau đó, code của phần này sẽ lấy input là một loạt các link và những commentary. Mong muốn output sẽ là một bài viết Markdown với các note về code, những nugget kiến thức, và một loạt các điểm mà chúng ta có thể rút ra từ những thay đổi. Mục đích của note này là để tìm hiểu những điều mới về Go và tổng hợp các tin tức xung quanh nó. Hãy viết system prompt riêng biệt để tạo ra bài viết như vậy.

**01:00:38** Ok, đoạn này là mô tả input là gì, output là gì, mục đích là gì. Cuối cùng là tạo prompt cho nó đúng không? Dạ, đúng rồi. Được rồi. Có Phát ở đây luôn à? Hay đấy. Ok, thử xem có nghĩ ra gì chưa... Chưa kỹ lắm, nhưng không sao. Mình có một cái system prompt đó là mình sẽ tạo một cái agent thực sự. Mình có thể tạo ra một workflow hoặc là chat flow, đơn giản hóa là mình muốn một cái tool cho Go để thay thế Phát, một agent để thay thế Phát. Tạm biệt Phát nhé. Ok, sau đó mình sẽ có tool rồi.

**01:01:53** Mình đã cài đặt sẵn một số tool cho team, bên phía anh Thành cũng có mention về GReader, đây là một cái tool giúp mình lấy thông tin từ mạng. Mình có tool này rồi, chắc mình sẽ lấy thông tin và chơi CL luôn, cho xịn. Được rồi, publish tạm trước đã, sau đó mình sẽ kéo các link vào. Chơi link của commentary mới nhất đi, commentary mới nhất là gì nhỉ? Đây rồi, ba cái CC này đúng không? Đang dùng GReader đúng không T? Dạ, GReader là tool kéo thông tin từ mạng, nhưng mà cái prompt để em điều khiển nó là gì? Hay là chỉ cần quăng link vào?

**01:02:52** Prompt là nhờ AI tạo cho mình rồi, như hồi nãy đúng không? Dạ, đúng rồi. Output là format bài viết của Phát, bao gồm nội dung cần thiết, code cần thiết, chi tiết đầy đủ, và có cả kết luận nữa. Cái này còn sâu hơn vì Phát thường không viết kết luận, nhưng mình có thể hình dung rằng AI agent sẽ lấy thông tin từ mạng, script nó, rồi đưa vào context của AI. Sau đó, AI sẽ dùng system prompt để tạo ra bài viết của Phát. Xem cái format này có giống không?

**01:04:00** So sánh thử xem.  Nhưng mình sẽ bỏ phần trên đi. Anh thử bật toc preview lên, xem nội dung nào ok. Có vẻ nội dung hơi nhiều quá, chắc phải lược bớt. Nếu muốn đi sâu hơn về code thì có thể nhờ AI chỉnh sửa code luôn. Ví dụ như đưa cái output đó cho chatbot. Đây là một ví dụ về output từ system prompt, thử làm cho nó ngắn gọn hơn, tập trung vào những điểm chính liên quan đến code.

**01:06:02** Chắc là hiểu sai prompt rồi. Không sao, dùng lại và recreate system prompt với những ý định đó. Thử lại lần nữa, giới thiệu lại cái link đó. Đây rồi, quá dễ, không tưởng tượng là bài của Phát dễ replicate như vậy. Ok, chắc bỏ cái này đi. Đọc thử xem nội dung thế nào. Nhưng chắc cần chỉnh sửa thêm. Hiện tại nó cover chính về telemetry, nhưng cái code kia chắc phải đi sâu hơn chút. Nhưng idea thì nó là như vậy thôi.

**01:08:26** Cách làm là từ cái plan của mình, có input, có mục đích của output là gì, và output mình muốn gần giống thế nào. Sau đó nhờ AI nó tự suy ra. Nó tạo ra bài viết giống bài của Phát. Thử thêm một câu nữa: Với kiểu này, anh muốn xài cái tone của thằng David Ruby. Đúng rồi, lấy danh sách các bài viết mới nhất của David Ruby, dùng phong cách viết technical của anh ta để viết lại phần commentary, xem thử output như thế nào.

**01:09:07** Đúng rồi, lấy luôn. Có thể nó sẽ nhiều hơn một chút, anh không biết bài nào chuẩn nữa. Ok, chờ luôn. Hoàn hảo rồi, bây giờ tối ưu hóa tone cho nó giống David Ruby. Đây là một ví dụ bài viết với tone của anh ta. Update system prompt để refactor lại giọng văn của David. Đây rồi, easy, game này dễ rồi. Thử làm lại xem nào. Hình như bài viết hơi bị bớt đi chút.

**01:11:55** Thỉnh thoảng nó có stop character xuất hiện. Chắc bị bug rồi. Nó biết là mình copy của người khác nên có thể liên quan đến copyright. Đây, bài này nói cái gì vậy? Ai biết đâu, thử mở xem. Chắc bị lỗi format, nhưng không sao, cứ chơi luôn. Ok, bỏ cái này đi, bật preview luôn. Gần xong rồi, nhưng chắc cần điều chỉnh thêm. Ok, phần này không có mấy keyword của David, có vẻ nó hơi đi chệch, nhưng không sao.

**01:13:34** Chắc do CL nó có filter keyword để tránh vi phạm bản quyền. Nó bỏ bớt một số keyword, nên chỉ nghe hơi giống thôi, chứ không hoàn toàn. Vấn đề này chắc không phải do thiếu sample mà là do system của CL nó xử lý khác. Đúng rồi, thôi bỏ qua cái này đi, cứ up cái mới. Sau đó cho một đoạn kết luận để xem.

**01:17:31** Ý tưởng là nếu có một agent có thể quét toàn bộ code, tới cuối tuần mình sẽ không cần viết báo cáo nữa. Ai đó chỉ cần review và thêm vào vài điểm thôi, rồi gửi báo cáo cho team. Như vậy dễ hơn rất nhiều. Thử với một hai iteration nữa thì chắc là ổn rồi. Xem lại một lần nữa xem. Đây, phần giữa có vẻ ok hơn. Nhìn vào bài này khá dễ đọc đúng không?

**01:18:37** Đúng rồi, tầm 85-90% giống với phong cách của David rồi. Chắc thêm một hai bài sample nữa thì sẽ ra được system prompt tốt. Ý tưởng là như vậy thôi, cuối cùng mình có thể sử dụng nó để tạo ra các bài viết mà không cần chỉnh sửa nhiều. Trước đây, OGIF community call không quan tâm nhiều đến system prompt, mà chủ yếu quan tâm đến output thôi. Vậy thì mình cứ chơi với output, đảm bảo nó đúng là được. Output ra thế nào thì kiểm tra cho đến khi nào mình thấy nó giống thì thôi.

**01:19:16** Chắc là mất khoảng 70-90% thì sẽ đạt. Vậy thì cứ clone lại giọng văn của Phát, xem thử agent có thể quét các bài viết mới nhất không. Nếu có thể chọn được đúng link phản ánh được hướng đi của thị trường, thì có thể tạo ra một cơ sở dữ liệu khác để theo dõi các thay đổi.

**01:20:11** Cái này mình có thể làm với API, gắn trên Dify. Thực ra mình đã tạo API rồi, bên phía Dify là một cái tool để kéo thông tin từ các link thú vị. Sau đó mình lọc các thứ liên quan đến Go, gửi link đó cho AI agent để nó viết lại. Cuối cùng nhờ Phát review, thêm ảnh là xong. Thực ra mình có thể nhờ AI tìm ảnh và so sánh xem có ổn không, nếu ổn thì để trên bài luôn. Step đó hơi rườm rà nhưng làm được. Ok, chắc để sau. Khi làm xong rồi thì Phát không cần viết Go commentary nữa, chỉ cần quản lý thôi.

**01:21:29** Nếu vậy thì tuần sau mình thử tiếp nhé. Hoặc là thử viết một agent để load bài viết hàng tuần cho team, tiết kiệm thời gian viết báo cáo.

**01:22:18** Nếu mình feed đầy đủ context cho nó thì nó sẽ giữ được format consistent và output sẽ đúng yêu cầu. Thử với dự án thật, lấy thông tin từ Discord, quản lý task và cả những phần thảo luận. Được không?

**01:24:01** Dễ mà, lần trước em đã giải thích về Spatio-temporal reasoning rồi. Cái này liên quan đến không gian và thời gian trong dự án thôi. Chắc là không cần standup meeting nữa. Tuần sau thử nhé, mình sẽ test thử với toàn bộ hệ thống của team.

**01:25:09** Ok, có vẻ không ai có câu hỏi gì thêm. Nếu mình hoàn thiện được cái này thì có thể tự động hóa được nhiều thứ hơn nữa, ngồi thiết kế thôi, agent sẽ chạy. Mọi người cứ thế mà nhận đề bài.

**01:26:05** Thầy vừa hỏi về con CloudZ system prompt thông qua một vài giao diện UI. So sánh giữa lib chat và tool này. Thực ra giống nhau thôi, prop kỹ thì input output giống nhau, chỉ khác là một cái chạy trên cloud, một cái trên local.

**01:26:05 N**ói chung là hơi giống nhau thôi, vì nếu nó giống nhau là do mình prop rất kỹ, mình prop kỹ về input, output. À đâu rồi, input, output và mục đích của cái output là gì thôi. Vì sao bên phía một số người online thấy AI có vẻ "ngu," nhưng thực sự là em chưa bao giờ thấy nó ngu cả, bởi vì prompt mà em đưa cho bên phía Cloud nó đầy đủ hết. Miễn là mình có thông tin đầy đủ thì dù mình để trên cloud hay trên console, hoặc trên lib chat, nó cũng như nhau thôi.

**01:26:41** Ý tôi là mấy anh em khác thấy AI ngu là do prompt của họ chưa tốt thôi đúng không? Dạ dạ. Anh Văn có một điểm là iOS 8.0 mới được, nhưng mà sử dụng GPT-4 thì sao, có ổn hơn không? Tí nữa trong đề xuất, nó có chuyện zoom ấy. GPT-4 thì em có thử rồi, nếu nói về nội dung viết bài thì được, nhưng liên quan đến deduction, toán học, hoặc category series thì nó không tốt bằng. Không biết tại sao, chắc là không được train kỹ về lý thuyết toán học, kiểu như Theory of Mathematics, nó không có đủ thông tin.

**01:27:35** Những ai đam mê toán sẽ biết ngay là GPT-4 không có nhiều dữ liệu về đó. Hiểu sơ sơ vậy thôi, nhưng thiếu data đó thì khi tạo system prompt liên quan đến toán học kỹ hơn, nó không làm được. Nhưng CL thì lại xịn hơn ở chỗ đó. Ok, Thành ơi, vậy thật ra bây giờ là làm sao để trong thời đại mà ai cũng có thể sử dụng AI dễ dàng, mình phải biết cách build cái set agent của mình trước. Tất cả anh em nào có khả năng build agent thì dễ rồi, nhưng dùng cái đó để làm tiếp, để maintain nó và áp dụng vào bài toán thì mới là cái chính.

**01:28:14** Trong ngắn hạn, đó là cái mà mình nên làm trước khi các team khác đuổi kịp. Nghe như bây giờ, automation nhanh lắm, kiểu visual hết trên mấy tool như Defile. Nhưng về khía cạnh công ty, các công ty khác vẫn còn làm manual nhiều lắm. Nếu anh em mình thành power user của hệ thống này, mình có set agent để điều khiển thì sẽ di chuyển rất nhanh và chi phí ship software ra sẽ thấp hơn rất nhiều. 

**01:29:00** Ừ, để tí nữa tôi mua. Nhưng mà anh Tom quăng lên cái là chá rồi mà? Dạ, dạ. Cái này thì dùng tài khoản của em, em có setup sẵn tài khoản tạm cho team rồi. Nếu team dùng nhiều hơn thì sẽ mở rộng thêm. Không sao đâu. 

**01:29:42** Nhưng nếu dùng cái này cho team mình thì tốt quá. Phải build set agent của mình cho các công việc hiện tại, rồi từ từ biến mọi người thành power user. Sau đó là ship những cái software khác có thời gian đào tạo thêm. Dạ, đúng là lộ trình phù hợp để nâng cấp team. Dạ dạ.

**01:30:37** Nhưng ví dụ, ngoài chuyện làm cho dự án thì mình cũng có thể optimize mấy cái kiểu chỉnh sửa bên phía chat hoặc bên phía open source. Nhờ AI sẽ làm hết. Đây là AI 100%, chạy trên cloud. Còn description thì mình không rõ lắm. Cái này là feature mới à? Feature mới này liên quan đến artifact. Hiện tại bên lib chat không hỗ trợ artifact. Ví dụ như mình share link này, share link của một cái code hoặc dự án, mở Private tab thì nó sẽ hiện artifact cho mình xem. Nhưng nhiều loại file thì phải hỗ trợ từng loại một.

**01:31:20** Cái này có code sẵn, dựa trên bài toán của ông đó. Lúc nó chạy Python hoặc React thì sẽ highlight và preview tương tự. Em có làm demo từ lúc không biết gì hết, xong rồi mất bao lâu để làm xong? Tóm lại là bao lâu? Ờ, một tiếng thôi, không đùa đâu. Một tiếng từ lúc đọc spec, hiểu code, sau đó chỉnh sửa design, sửa code và phần backend còn lại. Tổng cộng khoảng 2 đô, dưới 3 đô thôi.

**01:33:10** Cái này phải build ra một cái docker container, sau đó deploy trên server. Build lâu hơn code luôn. Vậy là chỉ mất 2 phút giới thiệu cái chatbot cho mọi người là đủ? Cần invite mọi người vào luôn nhỉ. Dạ, dạ. Anh coi phần của team rồi cấp tài khoản cho team là xong. Chắc em sẽ giải thích về lib chat: nó là clone của GPT nhưng cho phép mình chọn model, hiện tại hỗ trợ API của OpenAI và Anthropic. Nếu muốn dùng model hack thì cứ gắn API key vào thôi. Mình có UI để tương tác với chatbot, dùng cả file R và PDF.

**01:34:40** Sau này sẽ mở rộng cho team để track progress, kỹ năng lập trình AI, viết prompt. Cái này thực sự giúp cho việc lập trình nhanh hơn, có thể nhờ AI code luôn, nói chuyện với AI và nó hiểu ý mình, code hộ mình. Điểm hay của lib chat là có thể share link, ví dụ như mình đã nói ở trên. Ban đầu dùng Cloud nhưng Cloud không cho share link, nên ít nhất lib chat đảm bảo điều này. Cộng đồng lib chat cũng đã bắt đầu thừa nhận và pickup rồi. Star của dự án này bao nhiêu?

**01:35:52** Cũng lâu rồi, con này chỉ là UI thôi. Cộng đồng ngoài Danny còn có Cris và một vài người khác. Nhưng gọi là cộng đồng thì hơi quá, chỉ là một nhóm đam mê thôi. Còn ai làm giống thế này không, để mình đảm bảo là quen rồi thì có thể tự triển khai được luôn? Có lẽ là l chat có nhiều hơn, nhưng lớn nhất vẫn là l chat và lib chat.

**01:36:38** Vậy sao tôi không thấy con này nổi tiếng nhỉ? Nó double star à? Đúng rồi, nhưng chưa deploy rộng vì cần nhiều server, mà nhiều server thì nhiều tiền. Nên chi phí đó cứ claim cho anh Quang là được rồi. Ok, thử đi, có vẻ sẽ nhận được nhiều sự công nhận hơn. Về lâu dài thì có lợi đúng không? Dạ, đúng rồi. Nếu dùng để quản lý PDF thì cũng tốt, hoặc muốn xem artifact thì chuyển qua libre chat. Ok rồi, anh em còn có câu hỏi gì không?

**01:38:33** Chắc không hỏi gì nữa. Tô với cả team chắc sẽ test lại để xem con này chạy tới mức nào. Mình sẽ tranh thủ tuần sau test với team để mọi người quen tay. Dự án khác có data source khác nhau, nên có thể sẽ phải tích hợp nhiều hệ thống như Slack, Notion, hoặc Jira, mỗi nơi một kiểu.

**01:39:22** Notion với Slack thì dễ nhất, còn Jira thì hơi khó vì API của nó chưa tốt lắm. Chưa kể là thông tin dự án thì hay bị phụ thuộc vào PM, cũng khó để tóm tắt lại tất cả. Nhưng đủ thông tin rồi thì chắc không cần thêm gì nhiều nữa. Rồi, ok. Chắc mình kết thúc ở đây nhé. Hôm nay rất thú vị. Ok, cảm ơn Tom và mọi người. Hẹn gặp lại phần sau nhé!

---

**English Transcript**

**00:00** Alright, everyone is here, do we need anyone else, Tom? Anyone else to join? Can’t hear you. Ah, yeah, I’ll team up with Phát later on this. But do we need anyone else from the audience? Do we? It depends, we don’t really need anyone besides the people in the AI Club. Okay, so we’re good? We’re good, let’s gather everyone slowly and wait a bit more.

**15:44** So today, we have three main topics. First, the market report from Thành, then Phát’s part, which should be quick. The remaining time is for Tom. Looks like we still have plenty of time. Mỹ also has a lot to share, so if we run out of time, just post your stuff, Mỹ, and everyone can check it later. Mỹ, have you posted yet? Can everyone see the screen? Yeah, is the sound a bit low? Hang on, let me grab my phone. Hello, can you hear me now? Okay, we’re good now. Let’s get started. The first part is about BT.

**17:39** Aside from last week’s numbers, I’ll go through some key updates. First, starting next week, our team will implement two things. First, we’re enabling hybrid working. I’ll send a detailed link later, but right now, it’s mainly a walkthrough. The office has been revamped for those who find working from home too noisy or distracting. After the renovation…

**18:31** Currently, there are about five people using the office, and there’s space for about ten more. When you come here, we’ll cover the cost of using the Apple Studio Display, and we’ve fully equipped the office with Herman Miller chairs. If you’re unfamiliar with these chairs, you can Google them to learn more. Meeting rooms and other facilities have also been significantly improved. If anyone feels bored or uncomfortable working at home, they can consider coming to the office to work for one or two days a week. This is optional, not mandatory.

**19:48** But the team hopes everyone starts adjusting their work to gain essential benefits from working together at the office. This will help strengthen social relationships, especially during this period when there’s a large volume of knowledge and research to process. Working together will help speed things up, particularly as we approach the final stages of projects.

**20:28** Additionally, all parking and dinner costs will be covered. This is the main announcement, and Mỹ will write up a detailed notice for everyone soon. That’s the first announcement, and I think it’s something everyone will be interested in. The second announcement is about testing a new feature: Daily check-in. For those participating in Daily check-in, during the first two weeks, each check-in will reward you with about 3 to 5 ICY.

**22:29** That’s right, you’ll earn ICY daily. But in the initial phase, we’ll run it as a two-week trial to see how the feature works. Since office space is limited, we’ll prioritize those starting hybrid working, and if there’s leftover space, we’ll transition more people to remote work. We’re trying to bring the office environment to its best possible state for productivity.

**23:24** Because, in reality, office space is limited, the priority is encouraging those who want to come to the office a few days a week if they find working from home ineffective. If, after one or two weeks, everyone feels it’s working well, you can personalize your workspace, bringing personal items to make it more comfortable.

**24:04** Where does the Hanoi team check-in? Yeah, I’ve already brought my personal cabinet and some Lego sets from my kid to the office. So those are the two main announcements. Hopefully, everyone will consider and start testing it out. We’re trying to restore the physical working culture, with hybrid working days of 1-2 days a week depending on how everyone feels.

**25:15** Everyone will find that coming to the office will help foster more personal connections. You can come for half a day or a full day, depending on how you feel. Okay, that wraps up the important stuff. Now, I’ll hand the stage over to Thành to continue with the market report.

**26:08** Now, let’s go over some noteworthy articles from the past week. The top 20 articles our team read include the transition from DynamoDB to MySQL, and there was an interesting article about Neil that supported the research on Debox. Yesterday, people were discussing Debox quite actively on the team tech channel. The discussion was recorded, and you can check it out. But some points about Debox were unclear—it’s just a development box environment, not a deployment environment like containers.

**27:11** Debox is just a box for development environments, not for deployment like containers. In essence, we still need both environments because they don’t replace each other. On development environments, especially for M1 users, Debox installs a VM layer on the M1 machine to run the container. This is because Debox can’t run natively on M1, so it needs a VM as an intermediary layer. The solution NX team uses is Linux on VM, installed directly on the machine without a middle layer like other systems. That’s why this system works well on M1 machines.

**27:47** The next article is about Accessibility Design in Figma. The links for this article seem to be broken, Huy, can you check and fix them? Could you repost the correct links for the articles that people posted earlier? It looks like three links are broken, please fix those.

**29:12** As for YouTube, we don’t have any updates, but we do have updates on Discord. That’s right, React 19 and a logging system written in Go have been registered. This logging system is integrated with Relay App. Yesterday, someone registered for a demo, but it seems like they used the wrong link. Someone also posted about Data Engineering for Python, but that’s more of a replacement for Python. My system still uses Shell Script, not Python.

**30:38** Inno also posted a good article about OpenAI and copyright. That’s the news for the past week, and everyone can check out Tom’s reports. Tom’s reports have gathered a lot of key information this week. Three people have reposted links already—Tom, can you help post those articles again? They’re on our Facebook page. Everyone can check them out and increase engagement. Especially, the article Huy posted yesterday had some very neat and clean images, much more refined than before.

**32:48** Let’s see, can everyone see the screen? This week doesn’t have too much going on, only two main articles. The first one is about Script, a tool that allows you to write scripts in your Go program. It’s pretty straightforward to use, for example, you can read a file as a string, or count the lines that match a pattern. You can even embed Go functions into it. This tool is pretty new, and it offers a lot of customization features. You can make HTTP requests, or even customize your own functions.

**33:40** For example, the script below allows you to read a file and count the lines that match a certain string. Additionally, it supports embedding Go functions into the script. This is a new tool, similar to the Jam tool that some of you may know, but Jam is more for writing simple shell scripts. This tool doesn’t focus on UI, it’s mostly API-based.

**34:28** The second tool is about Go CH. Starting from version 1.23, you can toggle telemetry on or off. Telemetry is about collecting usage data from the app, for example, how the app is running, whether it crashes, etc. This data is sent to the Go team so they can monitor and improve Go. If you follow Go’s Slack channel, you may have seen some errors that were detected thanks to telemetry. With version 1.23, you can turn it off completely, or just leave it in local mode where it doesn’t send anything to the server.

**35:08** If you’re in Go’s Slack channel, you’ll see that some errors have popped up thanks to the telemetry data. So starting from version 1.23, you can toggle it on or off, or keep it in local mode if you don’t want to send data out. That’s about it for this week’s article.

**35:54** Phát had a question: is this code the system that Huy set up for our projects? I don’t know. Is it the error reporting system? Yeah, error reporting. But this error bot is a tool by the Go team, it sends error reports directly to the Go team. The main purpose is just to improve Go by detecting any issues.

**36:36** It captures bugs in the Go programming language, especially when the program crashes or when external tools like Go Please or Go RoboCheck encounter errors. It sends the error report directly to the Go team so they can catch and fix it. This system is just a tool for the Go team, it’s not a general-purpose tool for everyone to use.

**37:24** Phát asked whether we should enable this system. If enabled, we’ll be sending a small amount of data to the Go servers. However, I think it’s very small, just performance files. It also integrates with the profile-guided optimization system in Go that I mentioned last time. These files help improve the app’s performance without costing much.

**38:08** This data is mainly performance files, usually just a few files. If you don’t want to send any data, you can turn telemetry off completely. In production environments, we can set it to local mode so that nothing is sent out.

**38:51** The profile system that Phát mentioned last time is built once, and then it measures performance in subsequent builds. It checks how performance improves compared to the previous build. Thanks, Phát, for sharing that. Hằng, can you summarize the reports from last month, especially the key parts about system performance over the past month?

**40:10** Everyone can see that there’s quite a lot of information, but I think everyone has already grasped it. The new Wind system also has some significant improvements. As for errors, we’ll need to address those further. Has Quân Lê finished his part? Yes, it’s done, but I think we’re still missing some of the final files. Hằng, can you gather everything for the team to complete the report?

**41:43** Looking at the new AI tools this past month, there’s been some notable development, especially with tools like CR 3.5 and Sonnet. We’ve seen demos of their functionality before, but now they’ve gone beyond just demos. Some teams have started using them to develop apps. The CR 3.5 Sonnet tool was also tested by Amazon for upgrading Java systems to newer versions.

**43:12** According to an unverified report, this tool can handle about 80-90% of the work in upgrading systems, with significant cost savings. This is a sign that AI tools are starting to participate in software development more practically. Additionally, chatbots and other AI tools are now adding artifact-like features, allowing them to compile code snippets from user input and generate mini apps within chat environments.

**44:42** For example, with CR 3.5 Sonnet, there’s a form that allows users to calculate building costs. When data is input, the system generates an HTML snippet and creates a small web app to process the data. In the past, this was purely UI-related, but now it’s evolved into the ability to generate mini apps directly within the chat interface. Tô might be able to demo this for everyone later.

**45:28** Additionally, the ping system from Cross is being developed to save resources by reducing the need to re-request code that’s already been submitted. This feature helps reduce latency and improves processing speed.

**46:16** Another notable point is that OpenAI’s TT4 model now supports sure output, ensuring that the final output always conforms to the pre-defined schema. Previously, this depended a lot on how well the schema was defined, but now the system automatically checks and ensures that the final output is always correct. This is especially useful for systems that rely heavily on schemas and have a high error rate.

**47:31** For example, if we were developing an app like this, it would involve a lot of factors, like checking the correct selectors in the source. For instance, we need to input the correct selector for a link, know what its title is, and what its link is. To scan all the links correctly. Each website has a different structure, so how do we have a general solution? In reality, AI can help us understand the HTML structure of that website, understand how it organizes elements. And here’s a small tool to help with that. But we’ll need to ensure that the final output is always a list of links we’ve scanned.

In the past, doing this required many steps related to pinging. But now, with AI, we can define the schema in advance. In Visual Studio Code, for example, we can define the schema first, and ensure that the final output always includes the title, the link, and the description. This ensures that when we scan the links, everything will be correct. This is an example of the output when we scan links from Lobs or a similar website.

**48:44** This is a fairly important update from the past period that everyone should know about. Additionally, DVT4 now supports fine-tuning files, meaning we can fine-tune models based on our own personal data. This is useful when we want to train AI on our specific data for certain purposes. This feature is now available and can be used. There’s also a new module related to image processing and flagging, and some other features that I’m sure everyone has seen.

**49:27** Furthermore, there’s an update related to the UI, and our team is very excited about the V0 tool. This is a new tool that helps generate UI based on prompts. The initial results from this tool are very close to what we expected. Many teams have started using it to build mini apps. What’s worth noting is that V0 is currently being trained with two main frameworks, Svelte and TailwindCSS. This tool has been on the market for a while now, and our team has been using it for about a year. Other teams have also started integrating it into their processes.

**50:22** V0 has the advantage of having a very small library size. It’s based on Svelte UI and combined with TailwindCSS, so customizing the UI and components is very easy. The combination of these two technologies has made V0 a very popular choice for prototyping UI components. We expect that in the near future, V0 will take over the market share from major libraries like Material UI or Ant Design.

**51:10** Over the past month, there have been a few updates like that regarding tools and new features. Does anyone have any questions? In general, this month’s updates mostly revolve around tactical topics related to system upgrades and trending tools that big players like Amazon and Google are developing. So far, there haven’t been many updates about completely breakthrough products or technologies, but we’ve seen some progress.

**51:59** Recent news has started to show signs of deeper integration of AI systems into real applications. For example, Apple is integrating AI tools directly into their systems, and Amazon has removed Alexa to replace it with cloud-based AI. We don’t know what changes will come next, but the trend is clear that major companies are gradually incorporating AI into their processes comprehensively.

**53:01** Yesterday, when our team checked the email, Google also sent a notice asking if we wanted to enable AI for our team. They’re offering a service at around $15 per user, with various pricing tiers, but it’s quite expensive. We only use AI moderately, nothing too extensive.

**53:49** However, we can expect that AI service prices will decrease over time, similar to how last year, the cost of using GPT-3.5 was around $32, but this year, using GPT-4, the price has dropped by nearly half. The number of tokens is also increasing, leading to a decrease in cost. Perhaps in the near future, older methods like RNN (Recurrent Neural Networks) will become outdated, and AI will become more widespread.

**54:34** Tony Dinh’s team is also developing a system similar to the tool we’re using, but their speed isn’t as fast, so they probably won’t be able to compete. They’re selling a tool called TimingMind, a system that helps businesses use their own data to develop internal AI tools. Their goal is to integrate entire Office systems, work tools, and support services with AI.

**55:44** What’s interesting is that they’re racing to spread AI as quickly as possible, selling what they can sell right now. In the long run, AI will become an integral part of software products, just like the traditional software industry, with increasing demands for cost efficiency, performance, and customer service.

**56:48** Google’s team also has a very professional customer support service, using their own AI to interact with customers. It’s pretty impressive how they use their AI for this, and the result is very smooth, almost like talking to a real person. Alright, we’ll get the link for everyone to check and continue the discussion.

**57:37** Let’s have Phát join for pair programming with me. Today, we’ll create a workflow together. Honestly, I’ll be doing most of it, but I’ll give you a direction for how to think about programming this kind of app. Thành also mentioned that we want to create many apps written on the cloud; we can write many pretty cool apps like this. Today, we’re going to use Diffy. The plan today is to replace Phát with an AI agent that allows everyone to write apps like this themselves.

**58:23** Go commentary, initially, I have a plan to create a system prompt using AI, a system prompt with input as a link and output as a write-up and notes. We need an agent to fetch the link, summarize it, format it, and note it down. This is my thinking before starting the work. Let’s get to it, so everyone can see how this works. So we’re doing a Go commentary? Yes, exactly. Copy the entire thing, and let AI do it instead of Phát. Yeah, it can totally replace him.

**59:22** We’ll need AI to create that system prompt. I’ve created a chatbot, and we could use CL, but I’m using a library chat to create it for the AI Club. Today, it’ll be: “Help me create a system prompt to format notes in Markdown format with the following style...” After that, the code for this will take input as a series of links and commentary. The expected output will be a Markdown article with notes on code, nuggets of knowledge, and a series of points we can extract from the changes. The purpose of this note is to discover new things about Go and consolidate news around it. Write your system prompt separately to create the article like this.

**01:00:38** Okay, this part is about describing the input, output, and purpose. Finally, it’s about creating the prompt, right? Yeah, that’s right. Got it. Is Phát here? Cool. Okay, let’s see if we’ve come up with something... not super refined yet, but no problem. We have a system prompt where we’ll create a real agent. We can create a workflow or a chat flow, simplifying things to where we want a tool for Go to replace Phát, an agent to replace him. [Laughs] Goodbye, Phát. Okay, after that, we’ll have the tool.

**01:01:53**  I’ve already installed some tools for the team, Thành also mentioned GReader, which is a tool for fetching information from the network. We’ve got this tool, so we’ll grab the info and use CL for the cool stuff. Okay, publish it first, then pull the links in. Let’s use the latest commentary link, what’s the latest commentary? Here we go, these three CCs, right? You’re using GReader, right T? Yeah, GReader is the tool for fetching info from the network, but what’s the prompt to control it? Or do you just throw the link in?

**01:02:52** The prompt was created by AI, right? Like we said earlier? Yeah, exactly. The output is to format Phát’s article, including the necessary content, the necessary code, full details, and even a conclusion. This is actually deeper because Phát doesn’t usually write conclusions, but we can imagine that the AI agent will fetch info from the network, script it, then give it to AI’s context. After that, the AI will use the system prompt to create Phát’s article. Let’s see if the format looks right.

**01:04:00** Let’s compare. We’ll leave the top part out. Turn on toc preview, let’s see which content looks good. The content seems a bit too much, we’ll probably have to trim it. If we want to go deeper into the code, we can ask AI to adjust the code directly. For example, give that output to the chatbot. Here’s an example of output from the system prompt, let’s make it more concise, focusing on the key areas related to the code.

**01:06:02** I think it misunderstood the prompt, but no worries. Use this and recreate the system prompt with these intentions. Let’s try again, introduce that link again. Here it is, too easy, I can’t believe Phát’s article is this easy to replicate. Okay, let’s cut this part. Read through it, see how it is. We probably need some more adjustments. Right now, it mainly covers telemetry, but the code might need a bit more depth. But the idea is there.

**01:08:26** The way to do it is from our plan: we have input, we have the purpose of the output, and we have the output we want it to resemble. Then ask AI to figure it out. It’ll generate an article similar to Phát’s. Let’s add another sentence: with this kind of style, I want to use David Ruby’s tone. That’s right, let’s get the latest articles by David Ruby, and use his technical writing style to rewrite the commentary and see what the output looks like.

**01:09:07** That’s right, let’s do it. It might take a bit more content, but I’m not sure which article is the best example. Okay, let’s wait. Perfect, now optimize the tone to match David Ruby. Here’s an example article with his tone. Update the system prompt to refactor the voice of David. This is easy, this is a piece of cake. Try it again. Looks like the article is slightly truncated though.

**01:11:55** Sometimes it has stop characters. Probably a bug. It knows we copied someone else’s work, so there could be copyright issues. Here, what is this article saying? Who knows? Let’s open it. Looks like a format error, but no worries, let’s go ahead anyway. Okay, remove this, publish the preview. Almost done, but we’ll probably need some more adjustments. Okay, this part doesn’t have the key phrases from David, it’s a bit off, but that’s alright.

**01:13:34** Maybe because CL has a filter to avoid copyright violations, it strips out some keywords, so it only sounds somewhat similar, not fully. This issue isn’t due to a lack of samples, it’s because CL handles things differently. Exactly, let’s move on from this, just upload the new version. Then we’ll add a conclusion.

**01:17:31** The idea is, if we have an agent that can scan the entire codebase, by the end of the week, we won’t even need to write reports anymore. Someone will just review it and add a few points, then send the report to the team. It makes things much easier. Try with one or two more iterations and it should be fine. Let’s review it one more time. The middle part looks good. This looks pretty readable, right?

**01:18:37** Yeah, it’s about 85-90% close to David’s style. A couple more sample articles and we’ll have a really good system prompt. That’s the idea, in the end, we can use it to create articles without much editing. Before, in OGIF community calls, we didn’t care much about the system prompt, just the output. So we’ll keep focusing on the output and make sure it’s correct. We’ll check the output until it matches what we want.

**01:19:16** Probably 70-90% completion, then it’ll be good. Let’s clone Phát’s tone, and see if the agent can scan the latest articles. If it can choose the right links that reflect market trends, we can create a separate database to track changes.

**01:20:11** We can do this with an API, using Dify. I’ve already created an API, Dify is a tool for pulling info from interesting links. Then we filter the relevant parts about Go, send the link to the AI agent to rewrite it. Finally, Phát can review it and add images. Actually, we can ask AI to find images, compare them to see if they fit, and if they do, we can publish them directly. That step is a bit tedious, but doable. Okay, let’s leave that for later. Once this is done, Phát won’t need to write Go commentary anymore, just manage it.

**01:21:29** If that’s the case, next week we can try it out. Or we could try writing an agent to load weekly articles for the team, saving time on writing reports.

**01:22:18** If we feed it the full context, it’ll keep the output consistent and correct. Let’s try it with a real project, pulling info from Discord, task management, and discussions. Sound good?

**01:24:01** Easy, last time I explained about Spatio-temporal reasoning, right? This is just about space and time in the project. We probably don’t even need standup meetings anymore. Let’s test it next week with the entire system.

**01:25:09** Okay, seems like no one has any more questions. If we get this done, we can automate a lot more, just sit and design, let the agent run. Everyone can just get the tasks.

**01:26:05** The professor just asked about the CloudZ system prompt through a UI. Comparing between lib chat and this tool. They’re pretty much the same. If you write a detailed prompt with input, output, and the purpose of the output, it’ll work the same whether on the cloud, local, or in libre chat.

**01:26:05** In general, they’re pretty similar, because if they’re the same, it’s because we write a very detailed prompt. I prop the input, output, and the purpose of the output thoroughly. That’s why some people online think AI is "dumb," but I’ve never seen it act dumb because the prompt I give to Cloud is always complete. As long as we give it enough information, it doesn’t matter if it’s on the cloud, console, or lib chat, it works the same.

**01:26:41** What I mean is, when people say AI is dumb, it’s because their prompt isn’t good enough, right? Exactly. You mentioned something about iOS 8.0, but using GPT-4, is it better? Yeah, later on, it comes up in proposals, there’s something about zooming. GPT-4, I’ve tried it out. For content writing, it’s decent, but when it comes to deduction, math, or category series, it’s not as good. Not sure why, maybe it’s not trained on Theory of Mathematics or anything like that, it just doesn’t have enough information.

**01:27:35** Those who are into math will know that GPT-4 doesn’t have enough data about that. It understands it a little, but without that data, it can’t create a proper system prompt for more complex math problems. But CL is much better in that aspect. Okay, Thành, so now the goal is for everyone to be able to use AI easily. We need to know how to build our set of agents first. Everyone who can build agents will find it easy, but using that to keep going, to maintain and apply it to real problems, is the main thing.

**01:27:35** Those who are into math will know that GPT-4 doesn’t have enough data about that. It understands it a little, but without that data, it can’t create a proper system prompt for more complex math problems. But CL is much better in that aspect. Okay, Thành, so now the goal is for everyone to be able to use AI easily. We need to know how to build our set of agents first. Everyone who can build agents will find it easy, but using that to keep going, to maintain and apply it to real problems, is the main thing.

**01:28:14** In the short term, that’s what we need to do before other teams catch up. Hearing how fast automation is now, everything is visualized on tools like Defile. But from a business perspective, many companies still do things manually. If our team becomes power users of this system, and we have agents to control, we’ll move very quickly, and the cost of shipping software will be much lower. Right? Buy a Protot team account.

**01:29:00** Yeah, I’ll get it later. But Tom already threw it up, right? Yeah, yeah. I’ve set up an account for the team already, it’s just temporary. If the team uses more, we’ll expand it. No worries. You should rest, don’t worry about it, Bi. You’re overthinking.

**01:29:42** But if we use this for our team, it’s perfect. We have to build our set of agents for the current tasks, then gradually turn everyone into power users. After that, we’ll ship other software with more time to train. Yeah, this is the right roadmap for upgrading the team. Absolutely.

**01:30:37** But for example, beyond just working on projects, we could also optimize edits on chat or open source. AI will handle all of it. This is 100% AI, running on the cloud. As for the description, I’m not sure. Is this a new feature? The new feature is related to artifacts. Right now, lib chat doesn’t support artifacts. For example, you share this link, share a link of some code or project, open it in a private tab, and it will show the artifact. But for many types of files, it will need to support them one by one.

**01:31:20** This already has code, based on someone else’s problem. When it runs Python or React, it will highlight and preview similarly. I did a demo from when I didn’t know anything, and how long did it take to finish? How long did it take in total? About an hour, no joke. One hour from reading the spec, understanding the code, then adjusting the design, fixing the code, and the backend. In total, it was 2-3 dollars.

**01:33:10** I had to build a docker container and deploy it on a server. The build process took longer than the coding. So it only took 2 minutes to introduce the chatbot to everyone? We should invite everyone to join now. Yeah, I’ll give the team access later. Let me explain lib chat: it’s a GPT clone but allows us to choose the model. Currently, it supports OpenAI and Anthropic APIs. If you want to use model hacks, just connect your API key. We have a UI to interact with the chatbot, and we can use R and PDF files as well.

**01:34:40** Later, we’ll expand it for the team to track progress, AI programming skills, and writing prompts. This really helps with programming speed, you can even ask AI to code for you, talk to it, and it understands your needs, codes everything for you. The nice thing about lib chat is you can share links, like I mentioned earlier. Initially, we used Cloud, but Cloud didn’t allow link sharing, so at least lib chat ensures this. The lib chat community has already started picking it up and acknowledging it. How many stars does this project have?

**01:35:52** It’s been a while, this is just a UI project. Outside of Danny, there’s Cris and a few others. But calling it a community is a bit of a stretch, it’s more of a passionate group. Does anyone else do something similar so that when we get familiar with it, we can clone it ourselves? Maybe l chat has more, but the biggest ones are l chat and libre chat.

**01:36:38** So why haven’t I heard more about this? It’s double-starred? Yeah, but it hasn’t been deployed much because it needs more servers, and more servers mean more money. So I’ll claim the cost for you. Okay, give it a try, it looks like it’ll gain more recognition. In the long run, it’ll be beneficial, right? Yeah, definitely. If you’re managing PDFs, it’s great, or if you want to view artifacts, switch to lib chat. Okay, got it. Does anyone else have any more questions?

**01:38:33** Looks like no more questions. Tom and the team will probably test this to see how far it can go. We’ll make sure to test it with the team next week, to get everyone familiar. Other projects have different data sources, so we’ll likely need to integrate multiple systems like Slack, Notion, or Jira, each with its own setup.

**01:39:22** Notion and Slack are the easiest, Jira is a bit more difficult since its API isn’t great. Not to mention, project information tends to rely on the PM, so it’s hard to summarize everything. But once we have enough information, we shouldn’t need much more. Alright, okay. Let’s wrap it up here. Today was really interesting. Okay, thanks Tom and everyone. See you next time.
